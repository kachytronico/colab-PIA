# üìö Resumen de Estudio - Clustering y Aprendizaje No Supervisado
**Alumno:** Alfredo Ledesma Ruiz  
**Tarea:** PIA UD03 - Sistema de Recomendaci√≥n Netflix  
**Prop√≥sito:** Resumen para preparaci√≥n de examen

---

## üìñ √çndice
1. [Aprendizaje No Supervisado](#1-aprendizaje-no-supervisado)
2. [Preprocesamiento de Datos](#2-preprocesamiento-de-datos)
3. [Reducci√≥n de Dimensionalidad (PCA)](#3-reducci√≥n-de-dimensionalidad-pca)
4. [Algoritmos de Clustering](#4-algoritmos-de-clustering)
5. [M√©tricas de Evaluaci√≥n](#5-m√©tricas-de-evaluaci√≥n)
6. [Sistema de Recomendaci√≥n](#6-sistema-de-recomendaci√≥n)
7. [F√≥rmulas y Conceptos Clave](#7-f√≥rmulas-y-conceptos-clave)
8. [Buenas Pr√°cticas](#8-buenas-pr√°cticas)

---

## 1. Aprendizaje No Supervisado

### ¬øQu√© es?
- **Definici√≥n:** T√©cnica de machine learning donde el algoritmo aprende patrones en datos **sin etiquetas previas**.
- **Objetivo:** Descubrir estructuras ocultas, agrupar datos similares, reducir dimensionalidad.

### Diferencia con Aprendizaje Supervisado
| Supervisado | No Supervisado |
|-------------|----------------|
| Datos con etiquetas (y conocida) | Datos sin etiquetas |
| Predicci√≥n/Clasificaci√≥n | Descubrimiento de patrones |
| Ejemplo: Spam/No spam | Ejemplo: Agrupar clientes similares |

### Tipos principales
1. **Clustering:** Agrupar datos similares
2. **Reducci√≥n de dimensionalidad:** Simplificar datos manteniendo informaci√≥n
3. **Detecci√≥n de anomal√≠as:** Encontrar datos at√≠picos

---

## 2. Preprocesamiento de Datos

### 2.1 An√°lisis Exploratorio de Datos (AED)

**Pasos aplicados:**
```python
# 1. Informaci√≥n b√°sica
df.info()           # Tipos de datos, nulos
df.head()           # Primeras filas
df.describe()       # Estad√≠sticas b√°sicas

# 2. An√°lisis de nulos
df.isnull().sum()   # Contar nulos por columna

# 3. Cardinalidad (valores √∫nicos)
df['columna'].nunique()
```

**Decisiones tomadas:**
- ‚úÖ Rellenar nulos con "Unknown" (no borrar filas)
- ‚úÖ Identificar columnas de alta cardinalidad (director: 4500+ valores √∫nicos)
- ‚úÖ Detectar listas dentro de columnas (g√©neros separados por comas)

### 2.2 Ingenier√≠a de Caracter√≠sticas

**T√©cnicas aplicadas:**

#### A) One-Hot Encoding (variables categ√≥ricas)
```python
# Para g√©neros (multi-etiqueta)
generos = df['listed_in'].str.get_dummies(sep=', ')
# Resultado: columnas binarias (0/1) por cada g√©nero
```

**Cu√°ndo usar:**
- Variables categ√≥ricas sin orden
- M√∫ltiples valores por registro (multi-hot)

**Ventaja:** El algoritmo entiende categor√≠as como n√∫meros  
**Desventaja:** Aumenta dimensionalidad (muchas columnas nuevas)

#### B) Frequency Encoding (alta cardinalidad)
```python
# Para actores (4500+ actores √∫nicos)
actor_counts = df['lead_actor'].value_counts()
df['lead_actor_freq'] = df['lead_actor'].map(actor_counts)
```

**Cu√°ndo usar:**
- Alta cardinalidad (evita "explosi√≥n" de columnas)
- La frecuencia es informativa (actores estrella)

#### C) Feature Engineering num√©rico
```python
# Crear nueva variable: cantidad de actores
df['num_cast'] = df['cast'].apply(lambda x: len(x.split(',')))
```

**Ventaja:** Captura informaci√≥n sin aumentar mucho la dimensionalidad

### 2.3 Escalado de Datos

**StandardScaler (estandarizaci√≥n):**
```python
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
df_scaled = scaler.fit_transform(df)
```

**F√≥rmula:**
```
z = (x - Œº) / œÉ
```
- Œº = media
- œÉ = desviaci√≥n est√°ndar
- Resultado: media = 0, desviaci√≥n = 1

**¬øPor qu√© es necesario?**
- KMeans usa distancias euclidianas
- Sin escalar: variables con rangos grandes dominan
- Ejemplo: `release_year` (1900-2020) vs `is_tv_show` (0-1)

**¬°IMPORTANTE!** Siempre escalar **antes** de PCA y clustering.

---

## 3. Reducci√≥n de Dimensionalidad (PCA)

### ¬øQu√© es PCA?
**Principal Component Analysis (An√°lisis de Componentes Principales)**

- T√©cnica para reducir n√∫mero de variables (columnas)
- Mantiene la m√°xima informaci√≥n posible
- Crea nuevas variables (componentes principales) que son combinaciones lineales de las originales

### ¬øC√≥mo funciona?
1. Encuentra la direcci√≥n de mayor varianza en los datos
2. Proyecta los datos en esa direcci√≥n (componente principal 1)
3. Encuentra la siguiente direcci√≥n perpendicular con mayor varianza (componente 2)
4. Repite hasta tener tantos componentes como se necesite

### Implementaci√≥n
```python
from sklearn.decomposition import PCA

# Mantener 90% de la varianza
pca = PCA(n_components=0.90)
df_pca = pca.fit_transform(df_scaled)

# Ver varianza explicada
print(f"Varianza explicada: {pca.explained_variance_ratio_.sum()}")
```

**En nuestra tarea:**
- Entrada: 46 columnas
- Salida: 35 componentes principales
- Varianza conservada: 91.43%

### Ventajas de PCA
‚úÖ Reduce "maldici√≥n de la dimensionalidad"  
‚úÖ Elimina ruido y correlaciones  
‚úÖ Acelera algoritmos (menos columnas)  
‚úÖ Mejora visualizaci√≥n (2D o 3D)

### Desventajas
‚ùå Componentes principales no son interpretables (combinaciones matem√°ticas)  
‚ùå Pierde algo de informaci√≥n

---

## 4. Algoritmos de Clustering

### 4.1 K-Means

**Concepto:**
- Algoritmo de partici√≥n
- Divide datos en K grupos (clusters)
- Cada cluster tiene un **centroide** (centro)

**C√≥mo funciona:**
1. Elige K puntos aleatorios como centroides iniciales
2. Asigna cada punto al centroide m√°s cercano
3. Recalcula centroides (promedio de puntos asignados)
4. Repite pasos 2-3 hasta convergencia

**Implementaci√≥n:**
```python
from sklearn.cluster import KMeans

kmeans = KMeans(n_clusters=32, random_state=42, n_init=10)
kmeans.fit(df_pca)
labels = kmeans.labels_  # Etiquetas de cluster por cada punto
```

**Par√°metros importantes:**
- `n_clusters`: N√∫mero de grupos (K)
- `random_state`: Semilla para reproducibilidad
- `n_init`: N√∫mero de inicializaciones (elige la mejor)

**Ventajas:**
‚úÖ R√°pido y eficiente  
‚úÖ Escalable a grandes datos  
‚úÖ Centroides interpretables  

**Desventajas:**
‚ùå Hay que elegir K manualmente  
‚ùå Sensible a inicializaci√≥n  
‚ùå Asume clusters esf√©ricos (forma circular)  
‚ùå Sensible a outliers

**M√©todo del Codo:**
```python
# Probar diferentes valores de K
for k in range(2, 64):
    kmeans = KMeans(n_clusters=k)
    kmeans.fit(data)
    inercia.append(kmeans.inertia_)  # Suma de distancias al centroide

# Graficar y buscar el "codo" (donde la curva se aplana)
```

### 4.2 Agrupamiento Jer√°rquico (Agglomerative)

**Concepto:**
- Construye una jerarqu√≠a de clusters
- **Enfoque bottom-up:** Empieza con cada punto como cluster y va fusionando

**C√≥mo funciona:**
1. Cada punto es un cluster individual
2. Fusiona los 2 clusters m√°s cercanos
3. Repite hasta tener K clusters (o uno solo)

**Implementaci√≥n:**
```python
from sklearn.cluster import AgglomerativeClustering

jerarquico = AgglomerativeClustering(n_clusters=32)
labels = jerarquico.fit_predict(df_pca)
```

**Visualizaci√≥n: Dendrograma**
```python
from scipy.cluster.hierarchy import dendrogram, linkage

Z = linkage(data, method='ward')
dendrogram(Z)
```

**M√©todos de enlace (linkage):**
- `ward`: Minimiza varianza (el usado en nuestra tarea)
- `single`: Distancia m√≠nima entre clusters
- `complete`: Distancia m√°xima entre clusters
- `average`: Distancia promedio entre clusters

**Ventajas:**
‚úÖ No requiere especificar K de antemano  
‚úÖ Genera dendrograma (visualizaci√≥n √∫til)  
‚úÖ Mejor con clusters no esf√©ricos  

**Desventajas:**
‚ùå M√°s lento que K-Means  
‚ùå No escalable a muchos datos  
‚ùå Decisiones irreversibles (no puede "desfusionar")

### 4.3 DBSCAN (Density-Based Spatial Clustering)

**Concepto:**
- Clustering basado en **densidad**
- No requiere especificar K
- Detecta **outliers** (puntos de ruido)

**C√≥mo funciona:**
1. Define un radio `eps` alrededor de cada punto
2. Si hay `min_samples` puntos dentro del radio ‚Üí punto **core**
3. Puntos alcanzables desde core ‚Üí mismo cluster
4. Puntos aislados ‚Üí **ruido** (etiqueta -1)

**Implementaci√≥n:**
```python
from sklearn.cluster import DBSCAN

dbscan = DBSCAN(eps=3, min_samples=5)
labels = dbscan.fit_predict(df_pca)

# Contar ruido
n_noise = list(labels).count(-1)
```

**Par√°metros:**
- `eps`: Radio de vecindad (distancia m√°xima)
- `min_samples`: M√≠nimo de puntos para formar cluster

**Ventajas:**
‚úÖ No requiere K  
‚úÖ Detecta clusters de forma arbitraria  
‚úÖ Identifica outliers  

**Desventajas:**
‚ùå Dif√≠cil elegir `eps` y `min_samples`  
‚ùå No funciona bien en alta dimensionalidad  
‚ùå Problemas con densidades variables

**En nuestra tarea:**
- Gener√≥ 121 clusters (demasiada fragmentaci√≥n)
- Por eso se descart√≥

### 4.4 BIRCH (Modelo Opcional)

**Concepto:**
- **B**alanced **I**terative **R**educing and **C**lustering using **H**ierarchies
- Dise√±ado para grandes datasets
- Construye un √°rbol de caracter√≠sticas (CF Tree)

**Implementaci√≥n:**
```python
from sklearn.cluster import Birch

birch = Birch(n_clusters=32)
labels = birch.fit_predict(df_pca)
```

**Ventajas:**
‚úÖ Muy eficiente en memoria  
‚úÖ Maneja grandes vol√∫menes de datos  

**Desventajas:**
‚ùå Solo funciona con distancia euclidiana  
‚ùå Sensible a orden de datos

---

## 5. M√©tricas de Evaluaci√≥n

### 5.1 Coeficiente de Silueta (Silhouette Score)

**¬øQu√© mide?**
- Calidad del clustering
- Qu√© tan bien est√° un punto en su cluster vs otros clusters

**F√≥rmula:**
```
s(i) = (b(i) - a(i)) / max(a(i), b(i))
```
- `a(i)`: Distancia promedio al resto de puntos **en su cluster**
- `b(i)`: Distancia promedio al cluster **m√°s cercano**

**Rango de valores:**
- **+1**: Punto perfectamente asignado
- **0**: Punto en el borde entre clusters
- **-1**: Punto mal asignado

**Implementaci√≥n:**
```python
from sklearn.metrics import silhouette_score

score = silhouette_score(data, labels)
print(f"Silueta: {score:.4f}")
```

**Interpretaci√≥n:**
- 0.7 - 1.0: Excelente
- 0.5 - 0.7: Buena estructura
- 0.25 - 0.5: Estructura d√©bil pero razonable
- < 0.25: No hay estructura clara

**En nuestra tarea:**
- Jer√°rquico: 0.4397 (mejor)
- K-Means: 0.4095 (segundo)
- DBSCAN: 0.4026
- BIRCH: 0.3236

### 5.2 Inercia (Inertia)

**¬øQu√© mide?**
- Compactaci√≥n de clusters
- Suma de distancias al cuadrado de cada punto a su centroide

**F√≥rmula:**
```
Inercia = Œ£(distancia(punto_i, centroide_cluster_i)¬≤)
```

**Caracter√≠sticas:**
- Siempre disminuye al aumentar K
- Se usa en el **m√©todo del codo**

**Interpretaci√≥n:**
- Valores bajos = clusters compactos
- Buscar el "codo" donde deja de bajar significativamente

---

## 6. Sistema de Recomendaci√≥n

### 6.1 K-Nearest Neighbors (KNN)

**Concepto:**
- Encuentra los K puntos m√°s cercanos a un punto dado
- Usa distancias (euclidiana, Manhattan, etc.)

**Implementaci√≥n:**
```python
from sklearn.neighbors import NearestNeighbors

# 1. Crear modelo
nn = NearestNeighbors(n_neighbors=11, metric='euclidean')

# 2. Entrenar con datos del cluster
nn.fit(datos_cluster)

# 3. Buscar vecinos
distancias, indices = nn.kneighbors(punto_objetivo)
```

**Estrategia en nuestra tarea:**
1. **Filtrar por cluster:** Solo pel√≠culas del mismo cluster que el objetivo
2. **Filtrar por tipo:** Solo Movies (no TV Shows)
3. **KNN:** Buscar las 10 m√°s cercanas en distancia euclidiana

**Distancia Euclidiana:**
```
d = ‚àö[(x‚ÇÅ-x‚ÇÇ)¬≤ + (y‚ÇÅ-y‚ÇÇ)¬≤ + ... + (x‚Çô-y‚Çô)¬≤]
```

### 6.2 Ventajas de clustering + KNN
‚úÖ Reduce espacio de b√∫squeda (m√°s r√°pido)  
‚úÖ Agrupa pel√≠culas similares primero  
‚úÖ KNN solo busca dentro del cluster (m√°s preciso)

---

## 7. F√≥rmulas y Conceptos Clave

### Distancia Euclidiana
```
d(p,q) = ‚àö[Œ£(p·µ¢ - q·µ¢)¬≤]
```

### Estandarizaci√≥n (StandardScaler)
```
z = (x - Œº) / œÉ
Œº = media
œÉ = desviaci√≥n est√°ndar
```

### Varianza Explicada (PCA)
```
Varianza explicada = Œ£(varianza componentes) / Œ£(varianza total)
```

### Coeficiente de Silueta
```
s(i) = (b(i) - a(i)) / max(a(i), b(i))
```

### Inercia (K-Means)
```
J = Œ£Œ£ ||x·µ¢ - Œº‚±º||¬≤
```
- x·µ¢: punto i
- Œº‚±º: centroide del cluster j

---

## 8. Buenas Pr√°cticas

### Preprocesamiento
‚úÖ Siempre revisar nulos **antes** de borrar  
‚úÖ Rellenar con "Unknown" mejor que eliminar filas  
‚úÖ Frequency encoding para alta cardinalidad  
‚úÖ One-Hot solo para cardinalidad baja-media  
‚úÖ Escalar **antes** de clustering  

### Selecci√≥n de K
‚úÖ Usar m√©todo del codo (inercia)  
‚úÖ Usar coeficiente de silueta  
‚úÖ Probar potencias de 2 (8, 16, 32, 64)  
‚úÖ Buscar equilibrio: no muy pocos, no demasiados  

### PCA
‚úÖ Aplicar despu√©s de escalar  
‚úÖ Mantener 90-95% de varianza  
‚úÖ Documentar cu√°ntos componentes se generan  

### Evaluaci√≥n
‚úÖ Comparar m√∫ltiples algoritmos  
‚úÖ Usar silueta, no solo inercia  
‚úÖ Visualizar resultados (dendrograma, gr√°ficos de barras)  
‚úÖ Justificar elecci√≥n del modelo  

### Recomendador
‚úÖ Filtrar por cluster primero (eficiencia)  
‚úÖ Aplicar filtros de negocio (tipo Movie)  
‚úÖ Verificar que objetivo exista en datos  
‚úÖ Manejar casos edge (cluster con <10 elementos)  

---

## 9. T√©rminos Clave del Examen

| T√©rmino | Definici√≥n |
|---------|-----------|
| **Clustering** | Agrupar datos similares sin etiquetas previas |
| **Centroide** | Centro de un cluster (punto promedio) |
| **Inercia** | Suma de distancias al cuadrado dentro de clusters |
| **Silueta** | M√©trica de calidad de clustering (-1 a +1) |
| **PCA** | Reducci√≥n de dimensionalidad manteniendo varianza |
| **One-Hot Encoding** | Convertir categor√≠as a columnas binarias |
| **Escalado** | Normalizar datos a misma escala |
| **Dendrograma** | √Årbol jer√°rquico de fusiones de clusters |
| **Outlier** | Punto at√≠pico, alejado del resto |
| **KNN** | Algoritmo de vecinos m√°s cercanos |
| **Supervised Learning** | Aprendizaje con etiquetas (y conocida) |
| **Unsupervised Learning** | Aprendizaje sin etiquetas (descubrir patrones) |
| **Feature Engineering** | Crear nuevas variables a partir de datos |
| **High Cardinality** | Muchos valores √∫nicos en una columna |
| **Curse of Dimensionality** | Problemas con muchas dimensiones |

---

## 10. Preguntas T√≠picas de Examen

### 1. ¬øCu√°ndo usar K-Means vs Jer√°rquico?
**K-Means:**
- Datos grandes (millones de registros)
- Necesitas velocidad
- Clusters esf√©ricos

**Jer√°rquico:**
- Datos peque√±os-medianos (<10,000 registros)
- Quieres dendrograma
- Clusters de forma irregular

### 2. ¬øPor qu√© escalar los datos?
Porque algoritmos de clustering usan **distancias**. Sin escalar, variables con rangos grandes dominan el c√°lculo.

### 3. ¬øQu√© hace PCA?
Reduce dimensiones creando **nuevas variables (componentes)** que son combinaciones de las originales, manteniendo m√°xima varianza.

### 4. ¬øC√≥mo elegir K en K-Means?
1. M√©todo del codo (buscar inflexi√≥n en gr√°fico de inercia)
2. Coeficiente de silueta (probar varios K y elegir m√°ximo)
3. Conocimiento del dominio

### 5. ¬øQu√© indica una silueta de 0.4?
Estructura **razonable** pero no excelente. Los clusters est√°n definidos pero hay cierto solapamiento.

---

## 11. Checklist Pre-Examen

- [ ] S√© explicar qu√© es aprendizaje no supervisado
- [ ] Conozco la diferencia entre clustering y clasificaci√≥n
- [ ] Puedo explicar c√≥mo funciona K-Means
- [ ] Entiendo el m√©todo del codo
- [ ] S√© qu√© es el coeficiente de silueta y su rango
- [ ] Puedo explicar para qu√© sirve PCA
- [ ] Conozco cu√°ndo usar One-Hot vs Frequency Encoding
- [ ] S√© por qu√© es necesario escalar datos
- [ ] Puedo comparar K-Means vs Jer√°rquico vs DBSCAN
- [ ] Entiendo qu√© es un dendrograma
- [ ] S√© explicar distancia euclidiana
- [ ] Conozco ventajas y desventajas de cada algoritmo

---

## üìå Resumen Ultra-R√°pido

**Flujo completo:**
1. **AED** ‚Üí Entender datos
2. **Preprocesamiento** ‚Üí Limpiar, codificar, feature engineering
3. **Escalar** ‚Üí StandardScaler (media 0, std 1)
4. **PCA** ‚Üí Reducir dimensiones (90% varianza)
5. **Elegir K** ‚Üí M√©todo del codo + silueta
6. **Clustering** ‚Üí Probar KMeans, Jer√°rquico, DBSCAN
7. **Evaluar** ‚Üí Comparar siluetas
8. **Recomendar** ‚Üí Filtrar cluster + KNN

**Los 3 algoritmos obligatorios:**
- **K-Means**: R√°pido, centroides, K manual
- **Jer√°rquico**: Dendrograma, no escala, flexible
- **DBSCAN**: Detecta ruido, densidad, dif√≠cil ajustar

**M√©trica principal:** Coeficiente de Silueta (0 a 1, mayor mejor)

---

**¬°Buena suerte en el examen! üöÄ**
