---
title: PIA 03 — Resumen: Aprendizaje no supervisado y por refuerzo
source: PIA_03_Resumen teórico de la clase – Unidad 3_ Aprendizaje NO Supervisado y por Refuerzo.docx
---

### C1. Resumen teórico de la clase – Unidad 3: Aprendizaje NO Supervisado y por Refuerzo

#### 1. Introducción a los Modelos de Aprendizaje de la Unidad 3

Esta unidad marca un punto de inflexión en el módulo, ya que es la primera en la que abordamos directamente los modelos de inteligencia artificial. A diferencia de las unidades anteriores, centradas en el análisis exploratorio y el preprocesamiento de datos, ahora nos adentramos en dos paradigmas de entrenamiento que comparten una característica fundamental: ambos trabajan con datos **no etiquetados** . Esto significa que los modelos deben aprender a encontrar patrones y tomar decisiones por sí mismos, sin disponer de ejemplos previos con soluciones. A lo largo de la unidad, exploraremos cuatro tipos de aprendizaje fundamentales, de los cuales dos, el Aprendizaje No Supervisado y el Aprendizaje por Refuerzo, serán el foco principal de nuestro estudio.
1.1. Tipos de Aprendizaje Fundamentales
Para comprender el panorama del aprendizaje automático, es esencial diferenciar entre los cuatro paradigmas de entrenamiento más extendidos:
**Aprendizaje NO Supervisado** Este paradigma se caracteriza porque el modelo descubre la estructura inherente y los patrones ocultos en los datos sin requerir etiquetas ni ayuda humana. No se le proporcionan ejemplos de salida predefinidos. Siguiendo la analogía del tutor, el modelo no te dirá "esto es una manzana", sino que concluirá "este grupo de datos tiene características parecidas", permitiéndonos identificar patrones que agrupan manzanas, plátanos y mangos por su cuenta. Su objetivo es organizar y categorizar la información por sí mismo.
**Aprendizaje Supervisado** En este caso, el modelo aprende a partir de un conjunto de datos que incluye tanto las entradas como las salidas correctas, conocidas como etiquetas. Es como aprender con un "supervisor" que proporciona las respuestas. Una vez entrenado, el siguiente paso será como ponerle un examen al modelo y lo evaluaremos con datos que no ha visto nunca. Este enfoque se utiliza principalmente para dos tipos de tareas: la **clasificación** , que consiste en asignar categorías predefinidas (por ejemplo, "manzana" o "pera"), y la **regresión** , que busca predecir un valor numérico continuo (como "5,8").
**Aprendizaje Semisupervisado** Este enfoque es una mezcla de los dos anteriores. El modelo se entrena con una pequeña cantidad de datos etiquetados y una gran cantidad de datos sin etiquetar. Su objetivo es aprovechar la estructura de los datos no etiquetados para mejorar el rendimiento del aprendizaje, una técnica especialmente útil cuando el proceso de obtener etiquetas es costoso o complicado.
**Aprendizaje por Refuerzo** En este tipo de aprendizaje, un modelo, denominado **agente** , aprende interactuando con un entorno. El aprendizaje se basa en un proceso de prueba y error, donde el agente recibe **recompensas (un valor positivo) o un castigo (un valor negativo)** por sus acciones. A través de esta retroalimentación, el agente descubre por sí mismo las mejores estrategias para maximizar su recompensa total a lo largo del tiempo.A continuación, profundizaremos en el primer gran bloque de la unidad: el aprendizaje no supervisado.
#### 2. Profundización en Aprendizaje NO Supervisado

El aprendizaje no supervisado tiene una gran importancia estratégica, ya que permite extraer valor de grandes volúmenes de datos sin necesidad de un etiquetado previo. Su función principal es categorizar y agrupar datos en *clústers* para encontrar patrones y estructuras ocultas sin conocimiento previo. Entre sus aplicaciones prácticas más relevantes se encuentran la creación de sistemas de recomendación y la detección de anomalías. Por ejemplo, al analizar el comportamiento de los usuarios en una web, un modelo de agrupación podría detectar un grupo pequeño de usuarios con un comportamiento anómalo y descubrir así una brecha de seguridad que les permitía acceder a servicios sin pasar por el *login* .
2.1. El Algoritmo K-Means
El algoritmo K-Means es uno de los métodos más simples y utilizados en el aprendizaje no supervisado. Sus conceptos teóricos clave se pueden resumir en los siguientes puntos:
**Objetivo y Métrica de Distancia:** El objetivo de K-Means es crear grupos de datos cuyos miembros estén muy próximos entre sí. Para lograrlo, es indispensable definir una métrica de distancia que permita medir la cercanía entre los puntos. Las más comunes son la distancia *Euclídea* (la línea recta que une dos puntos) y la distancia *Manhattan* (que mide la distancia sin usar diagonales, como al moverse por las calles de una ciudad).
**Proceso Iterativo con Centroides:** El entrenamiento se basa en un proceso iterativo que utiliza **centroides** , que son puntos virtuales que representan el centro de masas de cada grupo. Este proceso repite dos pasos fundamentales hasta que los grupos se estabilizan:
**Asignación:** Cada punto de datos se asigna al grupo cuyo centroide se encuentre más cercano.
**Actualización:** La posición de cada centroide se recalcula, moviéndose al nuevo centro de masas de los puntos que ahora forman su grupo.
**Hiperparámetro Clave y el "Método del Codo":** El número de grupos a crear (el hiperparámetro *k* ) debe ser definido de antemano por el programador. Para determinar el valor óptimo de *k* , se utiliza comúnmente el **"método del codo"** . Esta técnica consiste en ejecutar el algoritmo con diferentes valores de *k* y medir la inercia (la velocidad con la que desciende el error). El caso extremo será cuando tengamos tantos centroides como puntos, donde la inercia será cero, y el peor caso será un único centroide, donde la inercia será máxima. El "codo" representa el punto donde añadir más clústers ya no aporta una reducción significativa del error.
**Requisito Fundamental:** Es crucial recordar que, siempre que se trabaje con modelos basados en distancias, se deben **estandarizar siempre los datos** . Esto evita que características con rangos de valores muy diferentes (por ejemplo, edad y salario) influyan de manera desproporcionada en los cálculos de distancia, asegurando que todas las variables contribuyan de forma equitativa.
2.2. El Algoritmo de Agrupamiento Jerárquico
El agrupamiento jerárquico es un método que construye una jerarquía de clústers de forma iterativa. Su principal método de visualización son los **dendogramas** , diagramas en forma de árbol que representan cómo los puntos o grupos más próximos se van uniendo progresivamente.Para medir la distancia no solo entre puntos, sino también entre grupos ya formados, se introduce un criterio adicional conocido como **"enlaces" (** ***linkage*** **)** . Existen cuatro tipos principales de enlace:
average: Mide la distancia desde el centro de masas (centroide) de cada grupo.
complete: Mide la distancia máxima entre los dos grupos, **tomando como referencia los dos puntos más alejados de cada uno.**
simple: Mide la distancia mínima entre los dos grupos, **usando como referencia los dos puntos más cercanos de cada uno.**
ward: Une los grupos que generan la **menor varianza posible** , buscando crear clústers muy compactos.
2.3. El Algoritmo DBSCAN
DBSCAN (Density-Based Spatial Clustering of Applications with Noise) es un modelo que se distingue por dos ventajas significativas frente a los anteriores: **no requiere especificar el número de grupos** a crear y es capaz de **detectar valores atípicos (** ***outliers*** **) de forma automática** .Su mecanismo de funcionamiento se basa en agrupar puntos según su densidad. A partir de un punto inicial, el algoritmo expande un clúster incluyendo a todos los puntos vecinos que se encuentren dentro de un **"rango de aceptación"** (un hiperparámetro denominado *eps* ). Los puntos que no pueden ser asignados a ningún grupo denso son clasificados como atípicos y reciben la etiqueta -1.Al ser un modelo basado en distancias, DBSCAN es vulnerable a la "maldición de la dimensionalidad", un fenómeno donde la distancia entre puntos se vuelve menos significativa a medida que aumenta el número de características. Por tanto, además de la estandarización, a menudo se recomienda aplicar **técnicas de reducción de la dimensionalidad** para obtener mejores resultados.Con esto concluimos nuestra exploración de los modelos no supervisados. A continuación, nos adentraremos en el segundo paradigma principal de la unidad: el aprendizaje por refuerzo.
#### 3. Profundización en Aprendizaje por Refuerzo

El aprendizaje por refuerzo se aleja del análisis de datos estáticos para centrarse en un paradigma mucho más dinámico. Este tipo de aprendizaje se basa en la idea de que tu modelo es como tu mascota: consiste en entrenar a un modelo inteligente, denominado **agente** , para que aprenda a tomar decisiones óptimas interactuando con un **entorno** mediante un sistema de prueba y error. El objetivo es que el agente descubra las mejores estrategias por sí mismo a través de la experiencia.
3.1. Componentes Clave: Agente y Entorno
Para comprender este paradigma, es esencial definir sus componentes clave:
**Agente:** Es el modelo de IA que observa el entorno y toma las decisiones (acciones) de forma autónoma.
**Entorno:** Es el contexto o "mundo" en el que el agente actúa y sobre el cual sus acciones tienen un efecto.
**Acción:** Es la decisión concreta que toma el agente en un momento dado dentro de un conjunto de posibilidades.
**Estado vs. Observación:** Es fundamental distinguir entre estos dos conceptos. Un **Estado** implica que el agente tiene información completa del entorno; es omnisciente, como en un juego de ajedrez donde conoce la posición de todas las piezas. En cambio, una **Observación** es una vista parcial del entorno en la que existe información oculta, como en un videojuego de plataformas donde solo se ve una porción del mapa en cada momento.
3.2. La Dinámica de Acciones y Recompensas
El ciclo de aprendizaje del agente se basa en una dinámica continua de interacción con el entorno:
**Espacio de Acciones:** Este término se refiere al conjunto de todas las acciones posibles que un agente puede realizar. Se diferencia entre un espacio **discreto** , con un número finito de acciones (como los botones de un mando de videojuego), y un espacio **continuo** , con infinitas acciones posibles (como girar el volante de un coche autónomo en cualquier ángulo).
**Recompensas y Castigos:** Por cada acción que realiza, el agente recibe una señal del entorno en forma de **recompensa** (un valor positivo) o un **castigo** (un valor negativo). El objetivo final del agente es maximizar la suma total de las recompensas que obtiene a lo largo del tiempo.
**El Dilema Central:** Durante su aprendizaje, el agente se enfrenta a un dilema fundamental que debe equilibrar constantemente:
**Exploración:** Probar acciones nuevas o no exploradas para descubrir si pueden conducir a una recompensa mayor en el futuro.
**Explotación:** Utilizar las acciones que, según su conocimiento actual, ya sabe que proporcionan la mejor recompensa conocida. El equilibrio entre ambas estrategias es clave para un aprendizaje exitoso, ya que una explotación excesiva puede impedir descubrir mejores soluciones, mientras que una exploración excesiva puede ser ineficiente.
3.3. Aplicación Práctica: Q-Learning
Un ejemplo práctico y muy conocido de aprendizaje por refuerzo basado en resultados numéricos es el método de **q-learning** .
El "cerebro" del agente en este método es una **Q-tabla** , una matriz que almacena la recompensa futura esperada para cada par (estado, acción) posible. Esta tabla guía al agente a la hora de decidir qué acción tomar en cada estado.
El aprendizaje se ajusta mediante dos parámetros clave que controlan cómo se actualiza la Q-tabla:
***Learning rate*** **(Tasa de aprendizaje):** Controla la rapidez con la que el agente actualiza su conocimiento con la nueva información que recibe. Un valor de 0 significa que el agente no aprende nada nuevo, mientras que un valor de 1 implica que descarta por completo la información antigua en favor de la nueva.
***Discount factor*** **(Factor de descuento):** Pondera la importancia que el agente le da a las recompensas futuras. Un valor cercano a 0 hace que el agente sea "miope", priorizando únicamente las ganancias a corto plazo. En cambio, un valor cercano a 1 lo vuelve "optimista", valorando más las acciones que pueden llevar a recompensas mayores a largo plazo. Por ejemplo, en el juego *Flappy Bird* , la acción de saltar para evitar un obstáculo recibe una recompensa inmediata a **corto plazo** . En cambio, en una partida de póker, hacer un *all-in* con una buena mano inicial es una acción cuya recompensa se espera a **largo plazo** , al final de la partida.
#### 4. Conclusiones de la Unidad

En esta unidad se han sentado las bases conceptuales de dos paradigmas fundamentales del aprendizaje automático: el aprendizaje no supervisado y el aprendizaje por refuerzo. Por un lado, hemos visto cómo los modelos no supervisados nos permiten encontrar estructuras y patrones ocultos en los datos, destacando la importancia de conceptos como las **métricas de distancia** para agrupar información similar. Por otro lado, hemos analizado cómo el aprendizaje por refuerzo permite crear **agentes inteligentes** capaces de aprender a tomar decisiones óptimas a través de la dinámica de **acción y recompensa** en un entorno interactivo. Estos paradigmas, cada uno a su manera, proporcionan herramientas fundamentales para extraer valor de los datos y construir sistemas inteligentes capaces de operar en entornos complejos del mundo real.
