---
title: PIA 02 — Resumen: Preprocesamiento de la información
source: PIA_02_Resumen de la unidad Preprocesamiento de la información.docx
---

### PIA_02_Resumen de la unidad Preprocesamiento de la información

1.0 Introducción: La Preparación Esencial de los Datos para la IA
En cualquier proyecto de Inteligencia Artificial (IA), los datos son el ingrediente fundamental. Sin embargo, al igual que un chef no puede crear un plato de alta cocina con ingredientes sin preparar, un modelo de IA no puede funcionar correctamente con datos en bruto. Los datos, tal como se recopilan, casi nunca están listos para ser utilizados: a menudo contienen valores nulos, información irrelevante o valores atípicos. La premisa fundamental es simple: para obtener **buenos resultados** , necesitamos **buenos datos** . El proceso de transformar estos datos crudos en un conjunto de información limpio, coherente y valioso se conoce como preprocesamiento. Este documento resume las técnicas clave para preparar datos tabulares antes de proceder al entrenamiento de un modelo.
2.0 Fases Clave del Preprocesamiento de Datos
El preprocesamiento de datos no es una acción única, sino una secuencia de fases estructuradas y metodológicas. Cada etapa aborda un tipo específico de problema presente en los datos crudos, preparando progresivamente el conjunto de datos para que sea comprensible y útil para un algoritmo de IA. Las siguientes secciones desglosan estas fases esenciales, ofreciendo una hoja de ruta clara para convertir la información en bruto en un activo de alta calidad para el modelado.
2.1 Limpieza Inicial: Columnas, Atípicos y Valores sin Sentido
El primer paso consiste en una limpieza exhaustiva del conjunto de datos para eliminar la información que es evidentemente incorrecta o inútil. Este proceso inicial se centra en tres áreas principales:
**Columnas inútiles:** Se refiere a características (columnas) que no aportan valor al problema, ya sea porque están repetidas, corrompidas o son una combinación de otras. La eliminación de una columna completa debe justificarse adecuadamente, ya que implica descartar información que pudo haber sido costosa de obtener.
**Valores sin sentido:** Son aquellos valores que son lógicamente imposibles, como registrar que "el día de la semana favorito es el día farola". Este tipo de datos debe ser eliminado siempre, sin excepción.
**Valores atípicos (Outliers):** Corresponden a valores que, aunque son posibles, se alejan significativamente de la norma del resto de los datos. Pueden distorsionar los resultados de un modelo y deben tratarse con cuidado. Existen tres estrategias principales para manejarlos:
***Estrategia NO permisiva:*** Se elimina cualquier valor que quede fuera de un intervalo de aceptación definido. Es útil cuando hay muy pocos datos atípicos (menos del 5%).
***Estrategia permisiva:*** Se conservan todos los valores, incluso los atípicos. Se aplica cuando hay muchos outliers o su eliminación compromete el entrenamiento del modelo.
***Estrategia semipermisiva:*** Se amplía el rango de aceptación para incluir más valores, buscando un equilibrio entre las dos estrategias anteriores.
2.2 El Reto de los Valores Nulos
Los valores nulos son, como los describe el material de origen, "el Freddy Krueger de cualquier programador". Un valor nulo representa un **valor desconocido** ; no es un cero ni una cadena de texto vacía. La mayoría de los modelos de IA no pueden procesar estos valores porque sus operaciones matemáticas subyacentes no están definidas para valores desconocidos, por lo que es imperativo definir una estrategia para gestionarlos. Las tres técnicas principales son:
**Eliminación:** La estrategia más simple consiste en eliminar cualquier fila que contenga un valor nulo. Aunque es fácil de aplicar, presenta un riesgo significativo: si el conjunto de datos tiene muchos nulos, se puede perder una cantidad drástica de información. No obstante, es una técnica útil para crear "Modelos Vanilla" iniciales que sirvan como punto de referencia.
**Imputación:** Este enfoque consiste en reemplazar los valores nulos con un valor estadístico calculado a partir de los datos existentes. Por ejemplo, se puede sustituir un nulo en la columna edad por la media de todas las edades conocidas, aunque se pueden usar otras métricas como la mediana (más robusta a valores atípicos) o la moda (para características categóricas).
**Predicción:** Una técnica más avanzada implica entrenar un modelo de IA secundario con el único propósito de predecir los valores faltantes. Este concepto, a veces llamado "modelception" o "matrioska de modelos", utiliza la potencia de la IA para rellenar los vacíos de información de manera inteligente.
2.3 Análisis y Codificación de Características
Una vez que los datos están limpios, el siguiente paso es analizar y transformar las columnas (o características) para que sean óptimas para el modelo. Esto implica dos tareas fundamentales: evaluar cuánta información realmente aporta cada columna y convertir los datos no numéricos a un formato que los algoritmos puedan entender.
**Análisis de variabilidad:** Esta técnica mide la cantidad de información que contiene una columna. Al igual que en el juego "quién es quién", el conjunto de datos será bueno cuando una característica te permita identificar el **máximo de instancias posible** . El principio es claro: las columnas con muy poca variabilidad (es decir, donde casi todos los valores son iguales) a menudo no ayudan al modelo a distinguir entre diferentes casos y son candidatas a ser eliminadas. Además, las columnas con una gran cantidad de valores nulos también pueden interpretarse como características de baja variabilidad y, en consecuencia, ser candidatas a la eliminación.
**Codificación de columnas categóricas:** Las columnas categóricas contienen un conjunto finito de valores de texto (p. ej., "lunes", "martes" o "rojo", "verde"). Como la mayoría de los modelos solo trabajan con números, estos valores deben ser codificados. El enfoque depende de si el orden de las categorías es relevante:
**Cuando el orden importa:** Para categorías ordinales, como los puestos en una carrera, se asignan números secuenciales (1, 2, 3...) que preservan la relación de orden inherente.
**Cuando el orden NO importa:** Para categorías nominales, como las ciudades, se utilizan técnicas que evitan introducir un orden artificial. Las más comunes son One-hot encoding (crea una nueva columna binaria por cada categoría), Binary encoding (usa códigos binarios para reducir el número de nuevas columnas) y Embedding encoding, una técnica avanzada del Procesamiento del Lenguaje Natural que se abordará en temas posteriores.
2.4 La Maldición de la Dimensionalidad
A lo largo del preprocesamiento, algunas técnicas reducen el número de columnas, mientras que otras, como el One-hot encoding visto en la sección anterior, pueden aumentarlo drásticamente. Este aumento es una de las causas más comunes de un problema conocido como **"La maldición de la dimensionalidad"** : el conjunto de problemas que surge al tener demasiadas características. Más datos no siempre es mejor; un exceso de columnas puede confundir al modelo en lugar de ayudarlo. Para mitigar esto, se utilizan dos enfoques principales:
**Técnicas de selección:** Estos métodos eligen un subconjunto de las columnas **originales** más relevantes para un modelo específico. Su principal ventaja es que **preservan la interpretabilidad** , ya que se sigue trabajando con las características originales. Esto es crítico en dominios regulados como la banca, donde puede ser necesario explicarle a un cliente por qué se le ha denegado un préstamo.
**Técnicas de extracción:** Estos métodos crean un conjunto de **nuevas** columnas, en menor cantidad, a partir de una combinación de las originales (ej., PCA, t-SNE). Son agnósticas al modelo, pero **sacrifican la interpretabilidad** , ya que las nuevas columnas abstractas no tienen un significado empresarial directo. Dado que estos métodos se basan en la varianza de los datos, es un paso previo fundamental escalar las características (mediante estandarización o normalización) para que todas operen en un rango común y ninguna domine el proceso por su escala.La elección entre selección y extracción representa un compromiso fundamental: ¿priorizamos la **interpretabilidad** del modelo, manteniendo las características originales, o buscamos el **máximo rendimiento predictivo** , incluso si eso significa trabajar con nuevas características abstractas cuyo significado empresarial se pierde? Dominar este flujo de trabajo, desde la limpieza inicial hasta la gestión inteligente de la dimensionalidad, es lo que distingue a un análisis de datos superficial de la construcción de una base sólida para la inteligencia artificial.
3.0 Conclusión: Construyendo la Base para Modelos de Calidad
En resumen, el preprocesamiento de la información no es un paso preliminar opcional, sino una habilidad indispensable y fundacional para cualquier profesional de la Inteligencia Artificial. La calidad de un modelo predictivo depende directamente de la calidad de los datos con los que se entrena. Las técnicas esenciales abordadas en esta unidad incluyen:
**Eliminación de columnas inútiles** , detección de valores sin sentido y tratamiento de atípicos.
**Tratamiento de valores nulos** mediante eliminación, imputación o predicción.
**Análisis de variabilidad** para medir el valor informativo de cada característica.
**Codificación de columnas categóricas** para convertir texto en representaciones numéricas adecuadas.
**Reducción de la dimensionalidad** para combatir los problemas derivados de un exceso de características.Dominar estos conceptos es el requisito previo para poder entrenar modelos de IA que sean robustos, precisos y, en última instancia, eficaces.
