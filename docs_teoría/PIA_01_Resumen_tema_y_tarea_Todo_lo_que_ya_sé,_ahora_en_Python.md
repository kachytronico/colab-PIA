---
title: PIA 01 — Resumen: Todo lo que ya sé, ahora en Python
source: PIA_01 Resumen tema y tarea Todo lo que ya sé, ahora en Python.docx
---

# UD01 Resumen: Todo lo que ya sé, ahora en Python

### 1. Introducción: El Salto a Python para la Inteligencia Artificial

Iniciar el camino en la Programación de Inteligencia Artificial (IA) implica una transición estratégica hacia las herramientas que definen el sector. Para un desarrollador con experiencia previa en lenguajes como Java, el paso a Python no es solo un cambio de sintaxis, sino una inmersión en el ecosistema donde la innovación en IA ocurre a diario. Python se ha consolidado como el lenguaje de facto para la mayoría de los desarrollos en este campo, y dominarlo es fundamental para implementar las técnicas más avanzadas.
Las razones que han posicionado a Python como el estándar de la industria de la IA son claras y se centran en su potente ecosistema y la preferencia de su comunidad global:
**Dominio en el Desarrollo:** Aunque existen alternativas como R, Java o C++, la gran mayoría de los proyectos, librerías y marcos de trabajo en IA se desarrollan y mantienen en Python.
**Comunidad y Colaboración:** La inmensa comunidad de desarrolladores y científicos de datos que utiliza Python garantiza un flujo constante de módulos, soluciones compartidas y soporte colectivo, acelerando el ciclo de desarrollo.
**Ecosistema Maduro:** Dispone de un conjunto de librerías especializadas que se han convertido en la base para cualquier proyecto de IA, facilitando desde la manipulación de datos hasta la construcción de complejos modelos de aprendizaje profundo.
Para aplicar estos conceptos de manera práctica, este curso se apoyará en un conjunto de herramientas específicas que forman el entorno de trabajo estándar de cualquier profesional de la IA.
### 2. Herramientas Fundamentales: Tu Entorno de Trabajo en IA

Dominar el entorno de desarrollo y las librerías fundamentales es el primer paso práctico para transformar la teoría en soluciones de IA funcionales. Este curso se centra en un conjunto de herramientas estándar en la industria que permiten una puesta en marcha rápida y eficiente, sin las barreras de la configuración local.
**Google Colab:** Será nuestro entorno de desarrollo principal. Se trata de una plataforma en línea que permite ejecutar código Python a través de "cuadernillos" o *notebooks* directamente en el navegador. Su principal ventaja es que elimina por completo la necesidad de instalar y configurar un entorno de desarrollo local, proporcionando acceso a recursos computacionales de forma gratuita. Todos los programas del curso se realizarán en cuadernillos de Colab.
**Numpy:** Es la librería fundamental para la computación numérica en Python. Permite trabajar con vectores y matrices de gran tamaño de una manera extraordinariamente eficiente, siendo la base sobre la que se construyen muchas otras herramientas de IA.
**Pandas:** Proporciona estructuras de datos de alto rendimiento y fáciles de usar, como los *DataFrames*, que son esenciales para la manipulación y el análisis de datos estructurados o tabulares. Es el punto de partida para casi cualquier tarea de IA que involucre datos organizados en tablas.
Estas librerías, junto con miles de otras creadas por la comunidad, se encuentran y gestionan a través de **PyPI** (Python Package Index), el gran registro de módulos de Python. Este ecosistema centralizado es una de las mayores fortalezas del lenguaje, ya que permite acceder e instalar herramientas para casi cualquier necesidad.
El dominio de estas herramientas no solo te proporcionará la capacidad técnica para desarrollar proyectos, sino que también te permitirá entender y colaborar con la inmensa comunidad global de IA que las utiliza a diario.
### 3. El Ecosistema de la IA: Comunidades y Recursos

El campo de la Inteligencia Artificial se caracteriza por ser excepcionalmente dinámico, competitivo y colaborativo. Su rápido desarrollo hace que sea crucial conocer las comunidades y plataformas clave, ya que son la principal fuente de conocimiento, herramientas y datos para el aprendizaje continuo y la especialización.
A continuación, se presentan las comunidades y plataformas más influyentes que todo desarrollador de IA debe conocer:
**OpenAI:** Conocida mundialmente por ser la organización creadora de modelos de lenguaje tan influyentes como ChatGPT, marcando el ritmo de la innovación en IA generativa.
**FastAI:** Es un ecosistema que comenzó como un módulo de IA y ha crecido hasta convertirse en una gran comunidad, enfocada en hacer el aprendizaje profundo más accesible.
**HuggingFace:** Se ha convertido en un repositorio central para la comunidad de IA. Es una plataforma indispensable para acceder a miles de *datasets* y modelos preentrenados, lo que permite acelerar el desarrollo de nuevas aplicaciones.
**Kaggle:** Una plataforma veterana y muy respetada, conocida por sus competiciones de ciencia de datos propuestas por empresas, que ofrecen premios a quienes logren entrenar los mejores modelos. Además, es una excelente fuente de *datasets* y cuadernillos compartidos por una activa comunidad de expertos.
Conocer dónde encontrar estos recursos y comunidades es tan importante como entender qué es la Inteligencia Artificial en sí misma.
### 4. ¿Qué es la Inteligencia Artificial? Un Vistazo a su Historia y Definición

En esencia, la **Inteligencia Artificial** es el campo de la informática que se centra en crear sistemas capaces de realizar tareas que, en condiciones normales, requieren de inteligencia humana. El objetivo principal de la IA es automatizar estos procesos para llevarlos a cabo de manera más eficiente y a una escala mucho mayor de lo que sería posible para una persona.
La historia de la IA ha sido una narrativa de grandes expectativas, profundos estancamientos y resurgimientos espectaculares. Su nacimiento formal se sitúa en la **Conferencia de Dartmouth de 1956**, donde se acuñó el término. Los primeros años vieron avances significativos como el desarrollo del **perceptrón**, la primera red neuronal. Esta fase inicial, conocida como la "primera ola de la IA", estuvo marcada por un gran optimismo que, al no cumplirse, desembocó en el primer "invierno de la IA". El campo resurgió con una "segunda ola de la IA" en los 80 y 90, con avances en aprendizaje automático. Sin embargo, la revolución más reciente llegó con el **aprendizaje profundo** (*deep learning*), consolidada por hitos como la arquitectura **Transformer**, presentada en el influyente artículo **"Attention is All You Need" (2017)** que transformó el procesamiento del lenguaje, y la popularización de los **modelos de difusión (2022)** para la generación de imágenes.
Esta evolución histórica ha dado lugar a una amplia variedad de enfoques y especializaciones, que se pueden clasificar según las tareas que abordan.
### 5. Tipos de Tareas en Inteligencia Artificial

La Inteligencia Artificial no es un campo monolítico, sino una disciplina con múltiples ramas y especializaciones. Para comprender mejor su alcance, es útil clasificar las tareas de IA según dos criterios principales: el tipo de aprendizaje que utiliza el modelo y el tipo de información que procesa.
#### Según el Tipo de Aprendizaje

Esta clasificación se enfoca en *cómo* un modelo adquiere su conocimiento, lo cual determina la estrategia de entrenamiento y el tipo de datos que necesitamos.
**Aprendizaje Supervisado:** El modelo aprende a partir de un conjunto de datos previamente etiquetado. Es decir, para cada entrada, se le proporciona la salida correcta esperada. Es el enfoque más común y se utiliza en tareas de clasificación (asignar una categoría) y regresión (predecir un valor numérico).
**Aprendizaje NO Supervisado:** En este caso, el modelo trabaja con datos sin etiquetar y su objetivo es descubrir patrones, estructuras o relaciones ocultas por sí mismo. Las aplicaciones típicas incluyen la agrupación de datos (*clustering*) o la detección de anomalías.
**Aprendizaje Semi-Supervisado:** Se utiliza cuando se dispone de una pequeña cantidad de datos etiquetados y una gran cantidad de datos sin etiquetar. El modelo aprovecha la estructura de todos los datos para mejorar su aprendizaje, siendo muy útil cuando el etiquetado es costoso, como en imágenes médicas.
**Aprendizaje por Refuerzo:** El modelo aprende interactuando con un entorno a través de un sistema de ensayo y error. Recibe recompensas por acciones correctas y castigos por las incorrectas, aprendiendo así la estrategia óptima para alcanzar un objetivo. Es muy utilizado en robótica, sistemas autónomos y videojuegos.
**Aprendizaje Autosupervisado:** El modelo aprende de datos no etiquetados creando sus propias etiquetas a partir de la estructura inherente de los datos. Se usa para entrenar modelos en tareas genéricas (pretexto) que luego se especializan en tareas más particulares, aprovechando el conocimiento general adquirido.
#### Según el Tipo de Información

Esta clasificación se centra en el *qué*, es decir, la naturaleza de los datos que el modelo analiza.
**Datos Estructurales:** Se trabaja con datos tabulares, como los que se encuentran en hojas de cálculo o bases de datos. Un ejemplo es la predicción del riesgo de infarto de un paciente a partir de su historial clínico.
**Imágenes (Visión por Computador):** El modelo analiza datos visuales. Las aplicaciones van desde la clasificación de objetos en una foto hasta la detección de tumores en imágenes médicas.
**Audio:** Se enfoca en el procesamiento de sonido. Los modelos pueden usarse para clasificar sonidos de animales en un ecosistema o para transcribir una conversación.
**Vídeo:** Combina el análisis de secuencias de imágenes y, a menudo, de audio. Se utiliza para tareas complejas como el seguimiento de objetos en movimiento o el reconocimiento de acciones.
**Texto (Procesamiento del Lenguaje Natural - PLN):** El modelo trabaja con lenguaje humano escrito. Este campo ha avanzado enormemente y permite construir *chatbots*, realizar resúmenes automáticos de documentos o traducir informes.
Es importante comprender que estos enfoques no son excluyentes. A menudo, se combinan modelos de diferentes tipos para resolver tareas más sofisticadas. Por ejemplo, la unión de modelos de texto e imagen permite generar imágenes a partir de descripciones textuales (como en DALL-E o Stable Diffusion), mientras que la combinación de modelos de texto y audio puede dar voz a un texto o transcribir una conversación.
### 6. Consideraciones Clave y Desafíos de la IA

El desarrollo de la Inteligencia Artificial no es solo un desafío técnico; conlleva profundas responsabilidades y presenta retos significativos que van más allá del código. Abordar estas cuestiones es fundamental para garantizar que la IA se desarrolle de una manera segura, justa y beneficiosa para la sociedad.
Los principales desafíos que enfrenta el campo actualmente incluyen:
**Ética y Sesgos:** Los modelos de IA aprenden de los datos con los que son entrenados. Si estos datos contienen sesgos sociales, históricos o de cualquier otro tipo, el modelo los aprenderá y los perpetuará, pudiendo llevar a resultados discriminatorios e injustos.
**Impacto en el Empleo:** La automatización de tareas cognitivas y manuales puede transformar el mercado laboral. Si bien esto puede desplazar ciertos empleos, también crea nuevas oportunidades y roles que requieren habilidades especializadas en IA y gestión de datos.
**Regulación:** Existe una necesidad crítica de establecer un marco regulatorio que encuentre el equilibrio adecuado entre fomentar la innovación y garantizar la seguridad. Una regulación efectiva es clave para prevenir usos maliciosos de la tecnología y proteger a los ciudadanos.
**Impacto Medioambiental:** El entrenamiento de los grandes modelos de IA, especialmente en aprendizaje profundo, consume una cantidad significativa de energía. La búsqueda de algoritmos más eficientes y de un hardware con menor consumo energético es un desafío activo para la sostenibilidad del sector.
Esta unidad didáctica ha sentado las bases conceptuales y de programación en Python necesarias para adentrarse en el mundo de la Inteligencia Artificial. Con una comprensión clara de las herramientas, el ecosistema, la historia y los desafíos del campo, ahora posees el marco conceptual y las herramientas para construir soluciones de IA innovadoras en los próximos módulos.
# Tarea ud_01 resuelta: Análisis Exploratorio de Datos del Dataset Titanic

## 1. El Desafío: Enunciado y Objetivos de la Tarea ud_01

Este informe presenta la resolución detallada de la tarea correspondiente a la Unidad 1 del curso "Programación de Inteligencia Artificial". El objetivo central es llevar a cabo un Análisis Exploratorio de Datos (AED) exhaustivo sobre un conjunto de datos seleccionado. A lo largo de este documento, se documentará cada fase del análisis, justificando las decisiones metodológicas tomadas para diagnosticar la calidad, estructura y características de los datos, demostrando cómo cada paso del análisis responde directamente a los criterios de evaluación y a las mejores prácticas de la industria.
### 1.1. Enunciado de la Tarea

La tarea exige la selección de uno de los conjuntos de datos disponibles en el módulo seaborn de Python. Una vez elegido, se debe ejecutar un Análisis Exploratorio de Datos completo. El enunciado pone especial énfasis en la necesidad de justificar cada acción y decisión tomada durante el proceso, desde la elección del dataset hasta la identificación de problemas como valores nulos, atípicos o redundancia de información.
### 1.2. Criterios de Evaluación

La evaluación de la tarea se rige por la siguiente rúbrica, que establece los puntos clave a desarrollar y su ponderación correspondiente:
Objeto de evaluación
Puntuación
Explicación y contextualización del conjunto de datos.
1 punto
Realización de un análisis de las columnas y argumentación de la eliminación (si procede) de las no deseadas.
2 puntos
Explicación de las características descriptivas más importantes del conjunto de datos.
1 punto
Uso de gráficos para mostrar la información relativa a las características del conjunto de datos.
2 puntos
Realización de un análisis de correlación entre las características y explicación de su posible correlación.
2 puntos
Estudio y detección de los valores atípicos del conjunto de datos.
2 puntos
A continuación, se detallan las herramientas y el entorno seleccionados para abordar estos criterios de manera sistemática y profesional.
## 2. Preparación del Entorno y Selección de Herramientas

La base de cualquier proyecto de ciencia de datos es un entorno de trabajo bien configurado y el uso de las herramientas adecuadas. Para este análisis, se ha utilizado Google Colab como entorno de desarrollo integrado, aprovechando su flexibilidad y acceso a recursos computacionales. Se importaron las librerías estándar de la industria, que constituyen el ecosistema fundamental para la manipulación, análisis y visualización de datos en Python.
### 2.1. Librerías de Python Utilizadas

Las siguientes librerías fueron seleccionadas por su robustez y funcionalidad específica, permitiendo cubrir todas las fases del AED de manera eficiente.
Librería
Alias
Propósito
**Pandas**
pd
Esencial para la manipulación y análisis de datos en formato de tabla (DataFrame).
**NumPy**
np
Necesaria para realizar operaciones numéricas eficientes con arrays y matrices.
**Matplotlib**
plt
Permite la creación básica de gráficos para la visualización de datos.
**Seaborn**
sns
Se utiliza para generar visualizaciones estadísticas avanzadas y para acceder a conjuntos de datos de ejemplo.
Con el entorno ya configurado, el primer paso práctico del análisis fue la selección de un conjunto de datos adecuado para los objetivos de la tarea.
## 3. Selección y Justificación del Conjunto de Datos

La elección del conjunto de datos es una decisión crucial en el AED, ya que define la naturaleza y la complejidad de los desafíos analíticos que se enfrentarán. Con el fin de seleccionar el caso de estudio más idóneo para cumplir con los objetivos de aprendizaje de la tarea, se evaluaron dos opciones candidatas, buscando un balance entre claridad y la presencia de problemas representativos del mundo real.
### 3.1. Exploración de Candidatos

Inicialmente, se utilizó la función sns.get_dataset_names() para listar todos los datasets disponibles en la librería seaborn. De esta lista, se procedió a cargar e inspeccionar dos conjuntos de datos para una evaluación preliminar: titanic y tips.
### 3.2. Análisis Comparativo

Se realizó un análisis estadístico descriptivo básico en ambos datasets utilizando el método .describe() para comprender sus características numéricas fundamentales.
#### Dataset 'titanic'

El análisis reveló que la **supervivencia media** fue del 38%.
Se detectó un problema de **valores nulos** en la columna age (edad), con solo 714 registros de un total de 891.
Se observó una gran disparidad en la columna fare (tarifa), donde el valor máximo (512.33) es significativamente superior al percentil 75 (31.0), sugiriendo la presencia de **valores atípicos**.
#### Dataset 'tips'

Este dataset se mostró más completo y con menos problemas de calidad evidentes a primera vista.
La variable total_bill (cuenta total) presentó una dispersión notable, con un valor máximo de 50.81 frente a una media de 19.78, lo que podría indicar la presencia de algunos valores atípicos, aunque menos extremos que en el caso de titanic.
### 3.3. Decisión Final

Tras el análisis comparativo, **se seleccionó el conjunto de datos titanic**.
La justificación de esta elección radica en su mayor complejidad y en los desafíos inherentes que presenta. La existencia de valores nulos significativos (age) y valores atípicos evidentes (fare) lo convierten en un caso de estudio mucho más rico y representativo de los problemas que se encuentran habitualmente en proyectos de Inteligencia Artificial. Abordar estos problemas permite aplicar un mayor número de técnicas de análisis y preprocesamiento, alineándose de manera más completa con los objetivos de la rúbrica de evaluación.
Una vez seleccionado el dataset, se procedió a realizar un análisis en profundidad.
## 4. Ejecución del Análisis Exploratorio de Datos (AED) sobre 'titanic'

El Análisis Exploratorio de Datos es un proceso sistemático e investigativo que tiene como fin diagnosticar la calidad, descubrir patrones y entender las características fundamentales de un conjunto de datos antes de proceder a la construcción de un modelo de Inteligencia Artificial. Las siguientes subsecciones detallan, paso a paso, la ejecución de este proceso sobre el dataset titanic, abordando cada uno de los puntos clave estipulados en la rúbrica de la tarea.
### 4.1. Diagnóstico Inicial: Estructura, Nulos y Tipos de Datos

La primera inspección del DataFrame se realizó utilizando el método .info(), que proporciona un resumen técnico de su estructura. Este diagnóstico es fundamental para obtener una visión global de la calidad de los datos, identificar problemas críticos como los valores faltantes y planificar las futuras tareas de preprocesamiento.
**Identificación de Valores Nulos:** El análisis de los valores no nulos por columna reveló los siguientes problemas:
Age (Edad): Presenta 177 valores nulos (714 de 891). Dada la importancia de esta variable, la estrategia a seguir será la **imputación** (relleno de datos) en una fase posterior, utilizando técnicas como la media o la mediana para preservar la mayor cantidad de información.
Deck (Cubierta): Con 688 valores nulos (solo 203 de 891), esta columna tiene más del 77% de sus datos faltantes. Una tasa tan elevada la convierte en una candidata clara para su **eliminación**, ya que su reconstrucción no sería fiable.
Embarked (Puerto de Embarque): Con solo 2 valores nulos, el impacto es mínimo. Estos pueden ser gestionados fácilmente mediante la eliminación de las filas correspondientes o la imputación del valor más frecuente.
**Identificación de Tipos de Datos:** El método .info() también permitió clasificar las variables según su tipo, lo cual es crucial para el modelado, ya que los algoritmos de IA requieren datos en formato numérico.
**Variables Categóricas (object):** Se identificaron las columnas sex, embarked, who y deck. Estas variables contienen texto y requerirán un proceso de **codificación** (transformación a números) en la fase de preprocesamiento (UD2) para que puedan ser utilizadas por un modelo.
**Variables Numéricas (int64, float64):** Se identificaron las columnas survived, pclass, age, sibsp, parch y fare. Aunque ya son numéricas, presentan rangos muy dispares. Esto sugiere que, en la UD2, podría ser necesario aplicar técnicas de **escalado o normalización** para evitar que variables con valores muy grandes (como fare) dominen el entrenamiento del modelo.
El diagnóstico de la columna Deck como prácticamente inservible nos lleva directamente a la siguiente decisión estratégica del análisis.
### 4.2. Estrategia de Limpieza: Argumentación para la Eliminación de Columnas

Aunque la acción de eliminar columnas se ejecuta formalmente en la fase de preprocesamiento (UD2), la decisión se fundamenta y argumenta durante el AED. Este paso consiste en identificar qué variables no aportan valor o podrían perjudicar el rendimiento del modelo.
**Columna Eliminada (Deck)**
**Problema:** La columna presenta un 77% de datos faltantes (688 de 891).
**Justificación:** Una tasa de nulos tan elevada es inmanejable. Cualquier intento de imputación introduciría un ruido y un sesgo inaceptables, comprometiendo gravemente la integridad del conjunto de datos. Por tanto, su eliminación es la decisión más lógica y segura.
**Columnas Mantenidas** Las demás columnas, a pesar de presentar ciertos problemas, se mantienen por las siguientes razones:
Las columnas Age y Embarked tienen un número de valores nulos manejable que puede ser tratado con técnicas de imputación.
Las columnas sex y who podrían contener información superpuesta. Sin embargo, su relación es entre variables categóricas, lo que requeriría un análisis específico (como una prueba de Chi-cuadrado) que está fuera del alcance de este AED inicial. Dado que no presentan problemas críticos de calidad de datos como la columna Deck, se decide retener ambas para un análisis más profundo en etapas posteriores.
Con las decisiones iniciales de limpieza argumentadas, el siguiente paso es utilizar herramientas visuales para profundizar en la comprensión de los datos.
### 4.3. Análisis Visual: Revelando Patrones a través de Gráficos

La visualización de datos es una herramienta poderosa en el AED. Permite comprender de manera intuitiva la distribución de las variables, la frecuencia de las categorías y la presencia de problemas como los valores atípicos, que a menudo son difíciles de detectar solo con métricas estadísticas.
#### Distribución de la Edad (Age) con Histograma

**Justificación:** Se utiliza un histograma para visualizar la distribución de una variable numérica clave como la edad. El objetivo es entender cómo se reparten los pasajeros por rangos de edad y verificar si la distribución se asemeja a una curva normal.
**Conclusiones:**
El gráfico revela un pico notable en la edad infantil (0-5 años).
La concentración principal de pasajeros se encuentra en el rango de adultos jóvenes (20-30 años).
La distribución no es perfectamente normal, lo que indica la heterogeneidad de la población a bordo.
#### Detección de Atípicos en la Tarifa (Fare) con Diagrama de Caja (Boxplot)

**Justificación:** Tras la sospecha inicial generada por el método .describe(), el diagrama de caja (boxplot) es la herramienta visual idónea para confirmar de manera inequívoca la presencia de valores atípicos en la variable fare.
**Conclusiones:**
El gráfico muestra una gran cantidad de puntos individuales por encima del "bigote" superior de la caja.
Esta observación confirma visualmente la existencia de numerosos *outliers* significativos, correspondientes a pasajeros que pagaron tarifas excepcionalmente altas.
#### Frecuencia de la Supervivencia (survived) con Gráfico de Barras

**Justificación:** Un gráfico de conteo (countplot) es la herramienta apropiada para visualizar la distribución de una variable categórica discreta, especialmente la variable objetivo (survived). Permite diagnosticar de inmediato problemas fundamentales como el desbalance de clases.
**Conclusiones:**
El gráfico confirma visualmente que el número de pasajeros que no sobrevivieron (clase 0) es considerablemente mayor que el de los que sí lo hicieron (clase 1).
Esto evidencia un **desbalance de clases** en la variable objetivo, un problema común que deberá ser abordado en la UD2 (por ejemplo, con técnicas de balanceo) para evitar que el modelo de IA aprenda a predecir mayoritariamente la clase dominante.
Del análisis visual de variables individuales, pasamos a analizar las relaciones entre ellas.
### 4.4. Análisis de Correlación: Identificación de Relaciones Numéricas

El análisis de correlación es una técnica estadística que mide la fuerza y la dirección de la relación lineal entre dos variables numéricas. Su propósito en el AED es detectar redundancia de información. Si dos variables están fuertemente correlacionadas, es posible que estén midiendo el mismo concepto subyacente, y mantener ambas podría perjudicar al modelo al darle un peso indebido a esa información.
**Interpretación de la Correlación:** Los valores de correlación oscilan entre -1 y +1. Un valor cercano a +1 indica una fuerte relación positiva (si una sube, la otra también), uno cercano a -1 indica una fuerte relación negativa (si una sube, la otra baja), y un valor cercano a 0 indica ausencia de relación lineal. Para este análisis, se estableció un **criterio de eliminación** para cualquier par de variables con una correlación superior a +0.8 o inferior a -0.8.
**Análisis del Mapa de Calor (Heatmap):** La matriz de correlación se visualizó mediante un mapa de calor, que facilitó la interpretación de los resultados:
**fare y pclass (-0.55):** Se observa una correlación negativa moderada, lo cual es lógico. A menor número de clase (pclass 1 es la más alta), mayor es la tarifa (fare) pagada.
**sibsp y parch (+0.41):** Existe una correlación positiva moderada, ya que ambas variables están relacionadas con el tamaño del grupo familiar del pasajero.
**Correlaciones con survived:** La clase (pclass, -0.34) y la tarifa (fare, +0.26) muestran las correlaciones más relevantes con la supervivencia, confirmando que los pasajeros de clases más altas y que pagaron más tuvieron mayores probabilidades de sobrevivir.
**Conclusión del Análisis:** Dado que **ningún coeficiente de correlación supera el umbral de ±0.8**, se concluye que no existe una redundancia crítica entre las variables numéricas. Por lo tanto, no se justifica la eliminación de ninguna de ellas por este motivo.
El último paso del análisis se centra en cuantificar matemáticamente los valores atípicos detectados visualmente.
### 4.5. Detección de Valores Atípicos: Cuantificación de Outliers en 'Fare'

Como paso final del AED, se procede a la identificación cuantitativa de los valores atípicos. Este paso va más allá de la inspección visual y utiliza un método matemático riguroso, el **Rango Intercuartílico (IQR)**, para confirmar y contar el número exacto de *outliers* en la variable fare, que ya habían sido identificados en el boxplot.
**Cálculo del Límite de Outliers:** Se aplicó el método IQR para definir el umbral a partir del cual un valor se considera atípico. Los resultados del cálculo fueron los siguientes:
**Cuartil 1 (Q1):** 7.91
**Cuartil 3 (Q3):** 31.00
**Rango Intercuartílico (IQR):** 23.09
**Límite Superior para Outliers:** 65.63 (calculado como Q3 + 1.5 * IQR)
**Resultados y Conclusión:** Al aplicar este límite, se identificaron un total de **116 valores atípicos** en la columna fare. Esta confirmación numérica es crucial, ya que valida las observaciones visuales y proporciona una cifra concreta del alcance del problema. Estos valores deberán ser gestionados adecuadamente en la fase de preprocesamiento (UD2) para evitar que sesguen el rendimiento de futuros modelos de IA.
Con este último punto, el Análisis Exploratorio de Datos se da por concluido, y se procede a resumir los hallazgos.
## 5. Conclusiones Finales del Análisis y Próximos Pasos

Este informe ha documentado la ejecución de un Análisis Exploratorio de Datos completo sobre el dataset Titanic, en cumplimiento con los requisitos de la tarea de la Unidad 1. El análisis ha permitido obtener un diagnóstico exhaustivo de la calidad, estructura y características de los datos, sentando una base sólida para las futuras fases de un proyecto de Inteligencia Artificial.
### 5.1. Resumen de Hallazgos Clave

El proceso de AED ha revelado los siguientes puntos críticos que deberán ser abordados:
**Presencia de Valores Nulos:** Se identificaron valores faltantes en la columna Age, que son manejables mediante técnicas de imputación, y en la columna Deck, cuya tasa de nulos es tan alta que justifica su eliminación completa.
**Necesidad de Codificación:** La existencia de variables categóricas clave como sex y embarked obliga a realizar una transformación a formato numérico (codificación) para que puedan ser procesadas por algoritmos de IA.
**Existencia de Valores Atípicos:** Se confirmó, tanto visual como analíticamente, la presencia de 116 *outliers* en la variable fare. Estos valores extremos podrían distorsionar el entrenamiento de un modelo si no se gestionan adecuadamente.
**Desbalance de Clases:** La variable objetivo survived presenta un desbalance significativo, con una mayor proporción de no supervivientes. Esto podría requerir el uso de técnicas de balanceo para asegurar que el modelo no se sesgue hacia la clase mayoritaria.
**Ausencia de Redundancia Numérica:** El análisis de correlación determinó que no existen variables numéricas con una correlación lo suficientemente alta como para justificar su eliminación por redundancia.
### 5.2. Próximos Pasos (UD2: Preprocesamiento)

Los hallazgos de este AED definen claramente la hoja de ruta para la siguiente fase del proyecto: el **preprocesamiento de datos (UD2)**. Las acciones a tomar incluirán:
La imputación de los valores nulos en la columna Age.
La gestión de los 116 valores atípicos detectados en fare.
La codificación de las variables categóricas para convertirlas en un formato numérico.
La posible aplicación de técnicas de escalado a las variables numéricas para normalizar sus rangos.
Este análisis ha cumplido su objetivo de diagnosticar el dataset, transformando un conjunto de datos crudo en una fuente de información comprendida y lista para ser preparada para el modelado.
vfc
